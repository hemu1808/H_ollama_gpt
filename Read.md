#HGPT RAG System - Complete Documentation
Enterprise-Ready Retrieval-Augmented Generation with Advanced Features

###Table of Contents
Overview
Key Features
Architecture
Prerequisites
Quick Start
Detailed Walkthrough
Configuration
Monitoring & Observability
Testing
Troubleshooting
Production Deployment
Scaling Guide
API Reference
Development Commands

##Overview
This is a production-ready Retrieval-Augmented Generation (RAG) system designed for enterprise document Q&A at scale. Unlike research prototypes, it includes:
Real BM25 keyword search (not fake TF-IDF)
Async everything for concurrent request handling
Redis caching with TTL and thread safety
Circuit breakers and retry logic for fault tolerance
Distributed BM25 or Elasticsearch for web-scale retrieval
Observability stack: Prometheus, Grafana, Jaeger
Security: JWT auth, rate limiting, input validation

##Key Features
Feature	Implementation	Benefit
Hybrid Search	Real BM25 + semantic with RRF fusion	30-40% better recall than single method
Multi-Query Retrieval	LLM generates 3 variations, runs parallel	Handles ambiguous queries
Semantic Chunking	Embeddings detect natural boundaries	Coherent chunks, no mid-sentence splits
Cross-Encoder Reranking	ms-marco-MiniLM-L-6-v2 reorders results	Precision @10 improves 25%
Agentic RAG	Self-reflection + follow-up searches	Answers complex multi-hop questions
Multi-Modal	Extracts PDF text, tables, images	Handles charts and tabular data
Graph RAG	Neo4j stores entity relationships	Structured reasoning over knowledge graph
Redis Caching	Async, thread-safe, persistent	10x latency reduction on repeated queries
Circuit Breakers	@circuit decorator on LLM calls	Prevents cascade failures
Rate Limiting	Per-user Redis counters	Survives abuse/denial-of-service
Observability	Prometheus metrics + Jaeger tracing	Full visibility into pipeline
Background Processing	Celery workers for large files	Async ingestion, no blocking API

###Architecture (High-Level Flow)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  User Request   ‚îÇ
‚îÇ  (HTTP POST)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FastAPI + Validation  ‚îÇ  ‚Üê‚îÄ‚îÄ Pydantic sanitizes input
‚îÇ Rate Limiter (Redis)  ‚îÇ  ‚Üê‚îÄ‚îÄ Checks per-user limits
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Query Expansion      ‚îÇ  ‚Üê‚îÄ‚îÄ LLM generates 3 variations
‚îÇ  (Circuit Breaker)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Parallel Retrieval   ‚îú‚îÄ‚î¨‚îÄ‚îÄ‚ñ∫  Semantic Search (ChromaDB)
‚îÇ  (Async Tasks)        ‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫  BM25 Search (Elasticsearch/Distributed)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RRF Fusion &         ‚îÇ
‚îÇ  Deduplication        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Cross-Encoder        ‚îÇ
‚îÇ  Reranking (GPU)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Context Compression  ‚îÇ  ‚Üê‚îÄ‚îÄ Truncate to token limit
‚îÇ  (tiktoken)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM Generation       ‚îÇ  ‚Üê‚îÄ‚îÄ Streaming response
‚îÇ  (Ollama)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Confidence Scoring   ‚îÇ  ‚Üê‚îÄ‚îÄ Multi-signal confidence 0-1
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Structured Response  ‚îÇ  ‚Üê‚îÄ‚îÄ JSON with answer, sources, metadata


Prerequisites
Hardware
CPU: 4+ cores (8+ recommended for production)
RAM: 16GB minimum, 32GB for 100K+ documents
GPU: Optional, 8GB VRAM for faster inference
Disk: 50GB+ for ChromaDB and PDF storage
Software
bash

 Install Docker & Docker Compose
 Ubuntu/Debian:
sudo apt-get update && sudo apt-get install docker.io docker-compose

 macOS:
brew install docker docker-compose

 Verify:
docker --version
docker-compose --version

 Install Python 3.11+ (if running locally)
sudo apt-get install python3.11 python3.11-venv
Model Downloads (Critical)
bash

 Pull Ollama models (takes 5-10 minutes)
ollama pull llama3.1:8b
ollama pull nomic-embed-text

 Pull cross-encoder (automatic but can pre-download)
pip install sentence-transformers
python -c "from sentence_transformers import CrossEncoder; CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
‚ö° Quick Start (5 Minutes)
Step 1: Clone & Setup
bash

git clone https://github.com/your-org/production-rag.git
cd production-rag
python -m venv venv
source venv/bin/activate   Windows: venv\Scripts\activate
pip install -r requirements.txt
Step 2: Configure
bash

 Edit .env file (copy from example)
cp .env.example .env
nano .env

 Essential settings:
OLLAMA_URL=http://localhost:11434
CHROMADB_PATH=./data/chroma
REDIS_URL=redis://localhost:6379
SECRET_KEY=change-this-in-production
RATE_LIMIT_PER_MINUTE=60
Step 3: Run Infrastructure
bash

 Start all services with Docker Compose
docker-compose up -d

 Verify all services are running
docker-compose ps

 Expected output:
 NAME                STATUS              PORTS
 rag_redis           Up                  0.0.0.0:6379->6379/tcp
 rag_chromadb        Up                  0.0.0.0:9000->8000/tcp
 rag_ollama          Up                  0.0.0.0:11434->11434/tcp
 rag_api             Up                  0.0.0.0:8000->8000/tcp
 rag_celery_worker   Up
 rag_prometheus      Up                  0.0.0.0:9090->9090/tcp
 rag_grafana         Up                  0.0.0.1:3001->3000/tcp
 rag_jaeger          Up                  0.0.0.0:16686->16686/tcp
Step 4: Verify Setup
bash

 Check health endpoint
curl http://localhost:8000/health

 Should return:
{
  "api": "healthy",
  "chroma": "healthy",
  "redis": "healthy",
  "ollama": "healthy"
}
Step 5: Try It!
bash

 Upload a PDF
curl -X POST http://localhost:8000/documents/upload \
  -H "Authorization: Bearer your_jwt_token" \
  -F "file=@/path/to/paper.pdf"

 Get task status
curl http://localhost:8000/tasks/celery-task-uuid-123

 Ask a question
curl -X POST http://localhost:8000/query \
  -H "Authorization: Bearer your_jwt_token" \
  -H "Content-Type: application/json" \
  -d '{"question": "What are the limitations of transformers?"}'
Access Web UIs:
Swagger UI: http://localhost:8000/docs
Grafana: http://localhost:3001 (admin/admin)
Prometheus: http://localhost:9090
Jaeger: http://localhost:16686
üîç Detailed Walkthrough: What Happens Under the Hood
Scenario 1: Uploading a Document
User Action: Upload transformer_paper.pdf (5MB, 50 pages)
Timeline (30-60 seconds total):
Table

Time	Component	Action
0s	FastAPI	Receives file, validates size (<50MB) and type (PDF)
0.1s	Security	Sanitizes filename, creates temp file with 0600 permissions
0.2s	Celery Task	Queues process_document_task with task_id
2s	Worker	Dequeues task, loads PDF with PyMuPDF
2-5s	Text Extraction	Extracts 50 pages ‚Üí ~800 sentences
5-10s	Semantic Chunking	Embeds each sentence (800 √ó 50ms = 40s, but parallelized to ~5s)
10-12s	Boundary Detection	Finds 120 semantic breakpoints (similarity < 0.7)
12-15s	Parent-Child Split	Creates 120 parents, 240 children
15-20s	ChromaDB Upsert	Async batch insert 240 vectors (HNSW index build)
20-25s	BM25 Index	Updates shards (DistributedBM25) or Elasticsearch bulk API
25-30s	Cache Cleanup	Invalidates Redis pattern emb:*
30s	Response	Returns {"task_id": "...", "status": "complete", "chunks": 240}
What Gets Stored:
ChromaDB: 240 vectors (768 dims each, float32) = 737KB vector data + HNSW structure
Redis: ~200 cached embeddings (from chunking) = ~15MB
BM25: Inverted index + document statistics = ~5MB
Scenario 2: Asking a Question
User Query: "What are the limitations of transformer models?"
Timeline (8-12 seconds total):
Table

Time	Component	Action
0s	FastAPI	Validates query (length 3-1000 chars, sanitizes HTML)
0.1s	Rate Limit	Checks Redis: rate_limit:user_123 < 60/min
0.2s	Query Expansion	Calls Ollama (5s timeout, circuit breaker)
0.5s	LLM Response	Returns 3 variations:
- "What are computational limits of transformers?"
- "What are the practical challenges of attention mechanisms?"
0.5-1s Parallel	Hybrid Search (3 queries √ó 2 methods)	Semantic:
- Embed query (50ms, cached)
- HNSW search (10ms) ‚Üí 40 docs each
BM25:
- Tokenize (1ms)
- Score shards (5ms) ‚Üí 40 docs each
1.1s	RRF Fusion	Combines 6 result lists (k=60), deduplicates ‚Üí 20 docs
1.2-1.5s	Cross-Encoder	Loads model, scores 20 pairs (15ms each) = 300ms CPU / 50ms GPU
1.5s	Context Building	Top 10 docs, truncates to token limit (tiktoken)
1.5-8s Streaming	LLM Generation	Ollama streams tokens (~30 tok/sec) ‚Üí 200 tokens = 6.7s
8s	Confidence Score	Computes weighted average: 0.79
8s	Response	JSON with answer, sources, confidence, metadata
Internal State:
JSON

{
  "latencies_ms": {
    "validation": 0.1,
    "rate_limit": 0.05,
    "query_expansion": 350,
    "hybrid_search": 580,
    "reranking": 320,
    "llm_generation": 6700,
    "total": 7950
  },
  "cache_stats": {
    "hits": 1,
    "misses": 2
  },
  "sources_used": 10,
  "confidence_signals": {
    "retrieval_confidence": 0.90,
    "context_overlap": 0.65,
    "answer_length": 0.85
  }
}
‚öôÔ∏è Configuration Reference (.env)
bash

 Application
APP_NAME=ProductionRAG
ENVIRONMENT=development
DEBUG=false

 API
API_HOST=0.0.0.0
API_PORT=8000
WORKERS=4   Number of Uvicorn workers

 Security
SECRET_KEY=change-this-to-a-secure-random-key
ALLOWED_HOSTS=["*"]   Restrict in production
RATE_LIMIT_PER_MINUTE=60   Per-user rate limit

 LLM
OLLAMA_URL=http://localhost:11434
OLLAMA_LLM_MODEL=llama3.1:8b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

 Vector Store
CHROMADB_PATH=./data/chroma
COLLECTION_NAME=production_rag
 For Docker Compose:
CHROMADB_HOST=chromadb
CHROMADB_PORT=8000

 Retrieval
DEFAULT_N_RESULTS=20   Docs retrieved before reranking
RERANK_TOP_K=10   Docs passed to LLM

 BM25
BM25_K1=1.5   BM25 term saturation parameter
BM25_B=0.75   BM25 document length parameter
BM25_SHARD_SIZE=10000   Docs per shard (DistributedBM25)
BM25_USE_ELASTICSEARCH=false   Set to true + configure ES
ELASTICSEARCH_URL=http://elasticsearch:9200

 Chunking
CHUNK_SIZE=512   Tokens per chunk
CHUNK_OVERLAP=50   Token overlap between chunks
SEMANTIC_CHUNK_THRESHOLD=0.7   Similarity breakpoint threshold

 Context Compression
ENABLE_CONTEXT_COMPRESSION=true
CONTEXT_COMPRESSION_RATIO=0.5   Keep top 50% sentences
MAX_CONTEXT_LENGTH=8192   LLM context window

 Redis
REDIS_URL=redis://localhost:6379
CACHE_TTL=3600   Seconds
EMBEDDING_CACHE_SIZE=100000   Max cached embeddings

 Background Processing
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

 Observability
PROMETHEUS_PORT=9090
JAEGER_ENDPOINT=http://jaeger:14268/api/traces
ENABLE_METRICS=true
ENABLE_TRACING=true

 Multi-Modal
ENABLE_MULTI_MODAL=false   Set true to enable table/image extraction
CLIP_MODEL=clip-ViT-B-32

 Graph RAG
ENABLE_GRAPH_RAG=false   Set true to enable Neo4j
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
üìä Monitoring & Observability
Metrics Available in Prometheus
Table

Metric	Type	Description	Alert Threshold
rag_query_duration_seconds	Histogram	Full query latency	p95 > 15s
retrieval_documents	Histogram	Docs retrieved per query	> 50 or < 5
ollama_errors_total	Counter	LLM errors by model	> 5% error rate
chroma_connection_failures_total	Counter	Vector DB failures	> 0
generation_tokens_total	Counter	Tokens generated	(cost tracking)
hallucination_rate	Gauge	Detected hallucinations/min	> 0.05
Grafana Dashboards
Access: http://localhost:3001 (admin/admin)
RAG Overview Dashboard
Queries/sec
Avg/p95/p99 latency
Confidence distribution
Error rate pie chart
Retrieval Deep Dive
Semantic vs BM25 contribution
Cache hit/miss ratio
Reranking score distribution
Top-K recall vs used
LLM Performance
Token generation rate (tok/sec)
Model load time
Queue depth (if using Ollama queue mode)
Per-model error breakdown
Jaeger Tracing
Access: http://localhost:16686
How to trace a query:
bash

 Add X-Trace-ID header
curl -X POST http://localhost:8000/query \
  -H "X-Trace-ID: debug-query-123" \
  -d '{"question": "test"}'
View trace: Search for debug-query-123 in Jaeger UI
Trace Structure:

Query Pipeline (8.2s)
‚îú‚îÄ‚îÄ validate_input (0.1ms)
‚îú‚îÄ‚îÄ rate_limit_check (0.05ms)
‚îú‚îÄ‚îÄ query_expansion (342ms)
‚îÇ   ‚îî‚îÄ‚îÄ ollama.chat (3 parallel calls)
‚îú‚îÄ‚îÄ hybrid_search (589ms)
‚îÇ   ‚îú‚îÄ‚îÄ semantic_search.parallel (3 queries)
‚îÇ   ‚îî‚îÄ‚îÄ bm25_search.parallel (3 queries)
‚îú‚îÄ‚îÄ rrf_fusion (3.2ms)
‚îú‚îÄ‚îÄ cross_encoder_rerank (298ms)
‚îú‚îÄ‚îÄ build_context (1.1ms)
‚îî‚îÄ‚îÄ llm_generate_streaming (6.9s)
üß™ Testing Suite
Unit Tests
bash

 Run all unit tests
pytest tests/unit/ -v --cov=core --cov=services

 Test specific component
pytest tests/unit/test_bm25_retriever.py -v

 With coverage report
pytest --cov-report html --cov=core
 View: open htmlcov/index.html
Integration Tests
bash

 Test full pipeline (requires services running)
pytest tests/integration/ -v

 Tests cover:
 1. Document upload ‚Üí retrieval ‚Üí answer
 2. Query expansion behavior
 3. Cache hit/miss scenarios
 4. Circuit breaker trip/recovery
 5. Rate limiting enforcement
Load Testing with Locust
bash

 Start Locust UI
docker-compose run --rm api locust -f tests/load_test.py --host=http://api:8000

 Open http://localhost:8089
 Set:
 - Users: 50
 - Spawn rate: 5/sec
 - Run time: 10m

 Expected results (4-core, 16GB, GPU):
 - RPS: 8-12 queries/sec
 - p95 latency: <12s
 - p99 latency: <15s
 - Error rate: <1%
 - CPU usage: 60-80%
Evaluation with RAGAS
bash

 Prepare evaluation dataset (JSONL format):
{"question": "What is BM25?", "answer": "BM25 is...", "contexts": ["..."]}

 Run evaluation
python -m evaluation.run_eval --dataset path/to/eval.jsonl

 Outputs metrics:
 - Faithfulness: 0.87 (target >0.85)
 - Answer Relevancy: 0.91 (target >0.90)
 - Context Precision: 0.76 (target >0.75)
 - Context Recall: 0.82 (target >0.80)
üö® Troubleshooting Common Issues
Issue 1: Ollama Model Not Found
Error: "model 'llama3.1:8b' not found"
bash

 Solution:
docker-compose exec ollama ollama pull llama3.1:8b
docker-compose exec ollama ollama pull nomic-embed-text

 Verify:
docker-compose exec ollama ollama list
Issue 2: ChromaDB Out of Memory
Error: RuntimeError: std::bad_alloc
bash

 Solution 1: Reduce HNSW parameters in .env
CHROMADB_HNSW_EF_CONSTRUCTION=50
CHROMADB_HNSW_EF=10

 Solution 2: Increase Docker memory limit
 Docker Desktop > Settings > Resources > Memory > 16GB
Issue 3: Redis Connection Refused
Error: ConnectionError: Error 111 connecting to localhost:6379
bash

 Check Redis status
docker-compose ps redis
docker-compose logs redis

 Restart Redis
docker-compose restart redis
Issue 4: Circuit Breaker Open
Error: CircuitBreakerError: Timeout
bash

 Check Ollama health
curl http://localhost:11434/api/tags

 If Ollama is down:
docker-compose restart ollama

 Monitor circuit state
docker-compose logs api | grep "circuit"
Issue 5: Poor Answer Quality
Symptom: Low confidence (<0.6) or irrelevant sources
bash

 Debug steps:
1. Check confidence score in response
2. Verify BM25 index is populated:
   docker-compose exec api python -c \
     "from services import RAGService; print(len(RAGService().bm25.corpus))"
3. If 0, rebuild index:
   docker-compose exec api python -c \
     "import asyncio; from services import RAGService; asyncio.run(RAGService()._rebuild_bm25_index())"
4. Check retrieval count:
   docker-compose logs api | grep "retrieval_documents"
Issue 6: Slow Query Performance
Symptom: p95 latency >15s
bash

 Profile a single query with Jaeger trace
 Identify bottleneck:
 - If LLM: Scale Ollama replicas or use GPU
 - If retrieval: Reduce DEFAULT_N_RESULTS
 - If reranking: Reduce RERANK_TOP_K or use GPU
üöÄ Production Deployment
Docker Compose (Single Server)
bash

 Production flags
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

 docker-compose.prod.yml includes:
 - Resource limits
 - Restart policies
 - Log aggregation
 - Health check intervals
Kubernetes (Multi-Server)
Step 1: Create namespace
bash

kubectl create namespace rag-system
Step 2: Apply configs
bash

 Secrets (API keys, passwords)
kubectl apply -f k8s/secrets.yaml

 ConfigMaps (env variables)
kubectl apply -f k8s/configmap.yaml

 Deployments
kubectl apply -f k8s/redis.yaml
kubectl apply -f k8s/chromadb.yaml
kubectl apply -f k8s/ollama.yaml
kubectl apply -f k8s/api.yaml
kubectl apply -f k8s/celery-worker.yaml
Step 3: Expose services
bash

 Load balancer for API
kubectl expose deployment rag-api --type=LoadBalancer --port=80 --target-port=8000 -n rag-system

 Get external IP
kubectl get svc -n rag-system
Step 4: Set up ingress (optional)
yaml

 k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: rag-ingress
spec:
  rules:
  - host: rag.yourcompany.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: rag-api
            port:
              number: 80
Terraform (Cloud Infrastructure)
hcl

 main.tf
module "rag_system" {
  source = "./modules/rag"
  
  region = "us-west-2"
  instance_type = "g5.xlarge"   GPU-enabled for Ollama
  desired_capacity = 3
  max_capacity = 10
}
üìà Scaling Guide
When to Scale
Table

Metric	Threshold	Scale What	To What
CPU usage >80%	5 minutes	API replicas	kubectl scale deployment rag-api --replicas=5
Memory >90%	1 minute	Node pool	Add 16GB RAM nodes
p95 latency >15s	10 minutes	Ollama replicas	Deploy 2 more Ollama pods with GPU
Redis memory >10GB	1 minute	Redis cluster	Enable cluster mode, add shards
ChromaDB latency >500ms	5 minutes	ChromaDB replicas	Deploy read replicas
BM25 search >1s	5 minutes	Elasticsearch	Scale to 3-node cluster
Horizontal Pod Autoscaler
yaml

 k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: rag-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: rag-api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
Apply:
bash

kubectl apply -f k8s/hpa.yaml
üéÆ Development Commands
bash

 Start development server (hot reload)
uvicorn api_main:app --reload --host 0.0.0.0 --port 8000

 Run tests with coverage
pytest --cov=core --cov=services --cov-report term-missing

 Lint code
black core/ services/ api/
isort core/ services/ api/
flake8 core/ services/ api/

 Type checking
mypy --strict core/
Makefile (convenience)
makefile

.PHONY: up down test lint fmt

up:
	docker-compose up -d

down:
	docker-compose down

test:
	pytest tests/ -v --cov

lint:
	flake8 core/ services/ api/

fmt:
	black core/ services/ api/
	isort core/ services/ api/

logs:
	docker-compose logs -f api

scale:
	docker-compose up -d --scale celery_worker=4
üîå API Reference
POST /query
Ask a question to the RAG system.
Request:
JSON

{
  "question": "What are the limitations of transformers?",
  "use_hybrid": true,
  "use_multi_query": true,
  "use_query_expansion": true
}
Response:
JSON

{
  "answer": "Transformer models have several limitations...",
  "sources": [
    {
      "source": "transformer_paper.pdf",
      "chunk_index": 12,
      "score": 0.92,
      "chunk_type": "child"
    }
  ],
  "confidence": 0.79,
  "metadata": {
    "query": "What are the limitations of transformers?",
    "search_method": "multi-query+hybrid",
    "latency_ms": 8250,
    "cache_hits": 2,
    "cache_misses": 1
  }
}
POST /documents/upload
Upload a PDF for processing.
Request:
bash

curl -X POST http://localhost:8000/documents/upload \
  -H "Authorization: Bearer token" \
  -F "file=@paper.pdf"
Response:
JSON

{
  "task_id": "celery-task-uuid-123",
  "status": "queued"
}
GET /tasks/{task_id}
Check document processing status.
Response:
JSON

{
  "task_id": "celery-task-uuid-123",
  "status": "SUCCESS",
  "result": {
    "chunks": 240,
    "documents_processed": 1
  }
}
GET /health
Health check endpoint.
Response:
JSON

{
  "api": "healthy",
  "chroma": "healthy",
  "redis": "healthy",
  "ollama": "healthy"
}
ü§ù Contributing
Fork the repository
Create feature branch: git checkout -b feature/new-retriever
Write tests: pytest tests/ -v
Run linting: make lint && make fmt
Submit PR: Include Jaeger trace and evaluation metrics
üìÑ License
MIT License - see LICENSE file for details
üôè Acknowledgments
ChromaDB team for vector storage
Ollama team for local LLM inference
Sentence Transformers for cross-encoders
RAGAS team for evaluation framework